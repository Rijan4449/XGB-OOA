{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rijan4449/XGB-OOA/blob/main/XGOOA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUhypzaDv6H1",
        "outputId": "04719928-bcc9-40d4-f5ec-6709106fbc9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt-mHmB6wng3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, log_loss, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import category_encoders as ce\n",
        "import xgboost as xgb\n",
        "from xgboost.callback import EarlyStopping\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import math\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXIyBuywwyyV",
        "outputId": "b1cdb9c6-ddd7-459e-e184-df1a0b530a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZywusAsbxMKS"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# 0. Seed\n",
        "# -----------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79fH9leSxZEF"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# 1. Load dataset\n",
        "# -----------------------\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/super_dataset.csv\")\n",
        "\n",
        "# PH waters test set\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/ph_waters.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ph_waters = {line.strip() for line in f if line.strip()}\n",
        "\n",
        "ph_test = df[df[\"waterbody_name\"].isin(ph_waters)]\n",
        "non_ph = df[~df[\"waterbody_name\"].isin(ph_waters)]\n",
        "test_size = int(0.2 * len(df))\n",
        "if len(ph_test) < test_size:\n",
        "    additional_needed = test_size - len(ph_test)\n",
        "    extra_non_ph = non_ph.sample(n=additional_needed, random_state=42)\n",
        "    test_set = pd.concat([ph_test, extra_non_ph])\n",
        "    train_set = df.drop(test_set.index)\n",
        "else:\n",
        "    test_set = ph_test\n",
        "    train_set = non_ph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVPm1sQVxfGW"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# 2. Define columns\n",
        "# -----------------------\n",
        "drop_cols = ['fish_id', 'common_name', 'status']  # note: we still drop 'status' later for final train\n",
        "high_cardinality = ['species', 'waterbody_name']\n",
        "low_cardinality = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'feeding_type']\n",
        "numeric_cols = [\n",
        "    'temp_max', 'weight_max', 'length_max', 'temp_pref_min', 'temp_pref_max',\n",
        "    'fecundity_mean', 'fecundity_min', 'fecundity_max',\n",
        "    'trophic_lvl_estimate_min', 'trophic_lvl_estimate_max', 'trophic_lvl',\n",
        "    'wb_ph_min', 'wb_ph_max', 'wb_salinity_min', 'wb_salinity_max',\n",
        "    'wb_do_min', 'wb_do_max', 'wb_bod_min', 'wb_bod_max',\n",
        "    'wb_turbidity_min', 'wb_turbidity_max', 'wb_temp_min', 'wb_temp_max'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoEpmu62xkO-"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# 3. Preprocessor\n",
        "# -----------------------\n",
        "numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median'))])\n",
        "low_card_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "high_card_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('target', ce.TargetEncoder())  # category_encoders TargetEncoder\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, numeric_cols),\n",
        "    ('low_cat', low_card_transformer, low_cardinality),\n",
        "    ('high_cat', high_card_transformer, high_cardinality)\n",
        "], remainder='drop')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQBT8BmAy5HM"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# 4. Risk discretization\n",
        "# -----------------------\n",
        "def discretize_risk(y):\n",
        "    bins = [0, 0.33, 0.66, 1.0]\n",
        "    labels = [0, 1, 2]  # 0=Low, 1=Medium, 2=High\n",
        "    return pd.cut(y, bins=bins, labels=labels, include_lowest=True).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAKSoTYpy-XD"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# 5. Fitness function for OOA/MOOA (unchanged)\n",
        "# -----------------------\n",
        "def fitness_function(params, X, y, preprocessor, n_splits=5):\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    cv_results = {\n",
        "        \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [],\n",
        "        \"roc_auc\": [], \"logloss\": [], \"train_accuracy\": []\n",
        "    }\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
        "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        y_tr_class = discretize_risk(y_tr)\n",
        "        y_val_class = discretize_risk(y_val)\n",
        "\n",
        "        # Preprocess: fit on train (with labels for target encoder) and transform\n",
        "        X_tr_num = preprocessor.fit_transform(X_tr, y_tr_class)\n",
        "        X_val_num = preprocessor.transform(X_val)\n",
        "\n",
        "        # SMOTE on processed numeric/encoded features\n",
        "        smote = SMOTE(random_state=SEED)\n",
        "        X_tr_res, y_tr_res = smote.fit_resample(X_tr_num, y_tr_class)\n",
        "\n",
        "        # Train XGBoost\n",
        "        model = xgb.XGBClassifier(\n",
        "            n_estimators=int(params[\"n_estimators\"]),\n",
        "            learning_rate=params[\"learning_rate\"],\n",
        "            max_depth=int(params[\"max_depth\"]),\n",
        "            subsample=params[\"subsample\"],\n",
        "            colsample_bytree=params[\"colsample_bytree\"],\n",
        "            gamma=params[\"gamma\"],\n",
        "            reg_lambda=params[\"reg_lambda\"],\n",
        "            objective=\"multi:softprob\",\n",
        "            eval_metric=\"mlogloss\",\n",
        "            min_child_weight=int(params[\"min_child_weight\"]),\n",
        "            reg_alpha=params[\"reg_alpha\"],\n",
        "            scale_pos_weight=params[\"scale_pos_weight\"],\n",
        "            use_label_encoder=False,\n",
        "            random_state=SEED,\n",
        "            verbosity=0\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            X_tr_res, y_tr_res,\n",
        "            eval_set=[(X_val_num, y_val_class)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_val = model.predict(X_val_num)\n",
        "        y_pred_tr = model.predict(X_tr_res)\n",
        "        y_val_probs = model.predict_proba(X_val_num)\n",
        "\n",
        "        # Metrics\n",
        "        cv_results[\"accuracy\"].append(accuracy_score(y_val_class, y_pred_val))\n",
        "        cv_results[\"precision\"].append(precision_score(y_val_class, y_pred_val, average=\"weighted\", zero_division=0))\n",
        "        cv_results[\"recall\"].append(recall_score(y_val_class, y_pred_val, average=\"weighted\"))\n",
        "        cv_results[\"f1\"].append(f1_score(y_val_class, y_pred_val, average=\"weighted\"))\n",
        "        cv_results[\"roc_auc\"].append(roc_auc_score(pd.get_dummies(y_val_class), y_val_probs, multi_class=\"ovr\"))\n",
        "        cv_results[\"logloss\"].append(log_loss(pd.get_dummies(y_val_class), y_val_probs))\n",
        "        cv_results[\"train_accuracy\"].append(accuracy_score(y_tr_res, y_pred_tr))\n",
        "\n",
        "    return np.mean(cv_results[\"roc_auc\"]), cv_results\n",
        "\n",
        "    # Final fitness: validation accuracy penalized by generalization gap\n",
        "    avg_val_acc = np.mean(cv_results[\"accuracy\"])\n",
        "    avg_gap = np.mean(cv_results[\"generalization_gap\"])\n",
        "    fitness_score = avg_val_acc - avg_gap  # Encourage generalizable models\n",
        "\n",
        "    return fitness_score, cv_results\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Helper: parameter bounds & utilities for MOOA\n",
        "# -----------------------\n",
        "param_bounds = {\n",
        "    \"n_estimators\": (500, 2500),\n",
        "    \"learning_rate\": (0.005, 0.15),\n",
        "    \"max_depth\": (6, 14),\n",
        "    \"subsample\": (0.65, 0.95),\n",
        "    \"colsample_bytree\": (0.65, 0.95),\n",
        "    \"gamma\": (0, 15.0),\n",
        "    \"reg_lambda\": (0.1, 40.0),\n",
        "    \"reg_alpha\": (0.0, 15.0),\n",
        "    \"min_child_weight\": (1, 30),\n",
        "    \"max_delta_step\": (0, 20),\n",
        "    \"scale_pos_weight\": (0.5, 15.0)\n",
        "}\n",
        "int_params = {\"n_estimators\", \"max_depth\", \"min_child_weight\", \"max_delta_step\"}\n",
        "\n",
        "def clamp_param(key, val):\n",
        "    lo, hi = param_bounds[key]\n",
        "    if key in int_params:\n",
        "        val = int(round(val))\n",
        "    return max(lo, min(hi, val))\n",
        "\n",
        "def agent_to_vector(agent):\n",
        "    keys = list(param_bounds.keys())\n",
        "    return np.array([agent[k] for k in keys], dtype=float), keys\n",
        "\n",
        "def vector_to_agent(vec, keys):\n",
        "    agent = {}\n",
        "    for i, k in enumerate(keys):\n",
        "        if k in int_params:\n",
        "            agent[k] = int(round(vec[i]))\n",
        "        else:\n",
        "            agent[k] = float(vec[i])\n",
        "        agent[k] = clamp_param(k, agent[k])\n",
        "    return agent\n",
        "\n",
        "# -----------------------\n",
        "# Helper: Lévy (Mantegna) and Brownian steps\n",
        "# -----------------------\n",
        "def levy_step(dimension, beta=1.5, scale=0.1):\n",
        "    # Mantegna's algorithm\n",
        "    sigma_u = (math.gamma(1 + beta) * math.sin(math.pi * beta / 2) /\n",
        "               (math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1.0 / beta)\n",
        "    u = np.random.normal(0, sigma_u, size=dimension)\n",
        "    v = np.random.normal(0, 1.0, size=dimension)\n",
        "    step = u / (np.abs(v) ** (1.0 / beta))\n",
        "    return scale * step\n",
        "\n",
        "def brownian_step(dimension, scale=0.02):\n",
        "    return np.random.normal(0, scale, size=dimension)\n",
        "\n",
        "# -----------------------\n",
        "# Helper: RFDB selection (fitness + distance)\n",
        "# -----------------------\n",
        "def rfdb_select(agents_vecs, agents_scores, global_best_vec, w_f=0.75, w_d=0.25, eps=1e-12):\n",
        "    scores = np.array(agents_scores, dtype=float)\n",
        "    # normalize fitness\n",
        "    if np.ptp(scores) < eps:\n",
        "        norm_f = np.ones_like(scores) / len(scores)\n",
        "    else:\n",
        "        norm_f = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "    # distances\n",
        "    dists = np.linalg.norm(np.stack(agents_vecs) - global_best_vec, axis=1)\n",
        "    if np.ptp(dists) < eps:\n",
        "        norm_d = np.ones_like(dists) / len(dists)\n",
        "    else:\n",
        "        norm_d = (dists - dists.min()) / (dists.max() - dists.min())\n",
        "    combined = w_f * norm_f + w_d * norm_d\n",
        "    combined = combined + eps\n",
        "    probs = combined / combined.sum()\n",
        "    idx = np.random.choice(len(probs), p=probs)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqTuAPbfzAXx"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# 6. MOOA Search (modified)\n",
        "# -----------------------\n",
        "def run_MOOA(X, y, preprocessor, max_iter=10, n_agents=20, random_state=SEED,\n",
        "             prob_explore=0.7, levy_prob=0.7, brownian_prob=0.3):\n",
        "    np.random.seed(random_state)\n",
        "    random.seed(random_state)\n",
        "\n",
        "    # initialize agents\n",
        "    agents = []\n",
        "    for _ in range(n_agents):\n",
        "        ag = {\n",
        "            \"n_estimators\": np.random.randint(param_bounds[\"n_estimators\"][0], param_bounds[\"n_estimators\"][1] + 1),\n",
        "            \"learning_rate\": np.random.uniform(*param_bounds[\"learning_rate\"]),\n",
        "            \"max_depth\": np.random.randint(param_bounds[\"max_depth\"][0], param_bounds[\"max_depth\"][1] + 1),\n",
        "            \"subsample\": np.random.uniform(*param_bounds[\"subsample\"]),\n",
        "            \"colsample_bytree\": np.random.uniform(*param_bounds[\"colsample_bytree\"]),\n",
        "            \"gamma\": np.random.uniform(*param_bounds[\"gamma\"]),\n",
        "            \"min_child_weight\": np.random.randint(param_bounds[\"min_child_weight\"][0], param_bounds[\"min_child_weight\"][1] + 1),\n",
        "            \"max_delta_step\": np.random.randint(param_bounds[\"max_delta_step\"][0], param_bounds[\"max_delta_step\"][1] + 1),\n",
        "            \"reg_alpha\": np.random.uniform(*param_bounds[\"reg_alpha\"]),\n",
        "            \"reg_lambda\": np.random.uniform(*param_bounds[\"reg_lambda\"]),\n",
        "            \"scale_pos_weight\": np.random.uniform(*param_bounds[\"scale_pos_weight\"])\n",
        "        }\n",
        "        agents.append(ag)\n",
        "\n",
        "    best_agent = None\n",
        "    best_score = -np.inf\n",
        "    best_cv_results = None\n",
        "\n",
        "    keys = list(param_bounds.keys())\n",
        "    agents_scores = [None] * n_agents\n",
        "    agents_vecs = [None] * n_agents\n",
        "\n",
        "    # Evaluate initial population\n",
        "    for i, ag in enumerate(agents):\n",
        "        vec, _ = agent_to_vector(ag)\n",
        "        agents_vecs[i] = vec.copy()\n",
        "        score, cv = fitness_function(ag, X, y, preprocessor)\n",
        "        agents_scores[i] = score\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_agent = deepcopy(ag)\n",
        "            best_cv_results = cv\n",
        "    print(f\"Init best ROC-AUC = {best_score:.4f}\")\n",
        "\n",
        "    # Main loop\n",
        "    for it in range(1, max_iter + 1):\n",
        "        best_vec, _ = agent_to_vector(best_agent)\n",
        "        new_agents = []\n",
        "        for i, ag in enumerate(agents):\n",
        "            vec, _ = agent_to_vector(ag)\n",
        "            dim = len(vec)\n",
        "\n",
        "            if np.random.rand() < prob_explore:\n",
        "                # Phase 1: Exploration\n",
        "                if np.random.rand() < levy_prob:\n",
        "                    step = levy_step(dim, beta=1.5, scale=0.08)\n",
        "                else:\n",
        "                    step = brownian_step(dim, scale=0.02)\n",
        "                scaled_step = np.zeros_like(step)\n",
        "                for j, k in enumerate(keys):\n",
        "                    lo, hi = param_bounds[k]\n",
        "                    rng = hi - lo\n",
        "                    scaled_step[j] = step[j] * rng\n",
        "                new_vec = vec + scaled_step * (np.random.rand(dim) * 0.5 + 0.5)\n",
        "            else:\n",
        "                # Phase 2: Exploitation (RFDB)\n",
        "                idx_partner = rfdb_select(agents_vecs, agents_scores, best_vec, w_f=0.75, w_d=0.25)\n",
        "                partner_vec = agents_vecs[idx_partner].copy()\n",
        "                w1 = np.random.rand()\n",
        "                w2 = np.random.rand()\n",
        "                new_vec = vec + w1 * (partner_vec - vec) + w2 * (best_vec - vec)\n",
        "                new_vec = new_vec + brownian_step(dim, scale=0.005) * np.array([param_bounds[k][1] - param_bounds[k][0] for k in keys])\n",
        "\n",
        "            candidate_agent = vector_to_agent(new_vec, keys)\n",
        "            score_candidate, cv_candidate = fitness_function(candidate_agent, X, y, preprocessor)\n",
        "\n",
        "            accept = False\n",
        "            if agents_scores[i] is None or score_candidate > agents_scores[i]:\n",
        "                accept = True\n",
        "            else:\n",
        "                if np.random.rand() < 0.05:\n",
        "                    accept = True\n",
        "\n",
        "            if accept:\n",
        "                new_agents.append(candidate_agent)\n",
        "                agents_scores[i] = score_candidate\n",
        "                agents_vecs[i] = agent_to_vector(candidate_agent)[0]\n",
        "                if score_candidate > best_score:\n",
        "                    best_score = score_candidate\n",
        "                    best_agent = deepcopy(candidate_agent)\n",
        "                    best_cv_results = cv_candidate\n",
        "            else:\n",
        "                new_agents.append(ag)\n",
        "\n",
        "        agents = new_agents\n",
        "        print(f\"Iteration {it}/{max_iter}, Best ROC-AUC = {best_score:.4f}\")\n",
        "\n",
        "    return best_agent, best_score, best_cv_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfuxxCqczKMf",
        "outputId": "d2f8aba5-f30b-4fab-c938-374eaaadcdb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init best ROC-AUC = 0.9322\n",
            "Iteration 1/20, Best ROC-AUC = 0.9345\n",
            "Iteration 2/20, Best ROC-AUC = 0.9345\n"
          ]
        }
      ],
      "source": [
        "# -----------------------\n",
        "# 7. Run MOOA\n",
        "# -----------------------\n",
        "start_time = time.time()\n",
        "best_params, best_score, best_cv_results = run_MOOA(\n",
        "    train_set.drop(columns=[\"invasion_risk_score\", \"status\"]),\n",
        "    train_set[\"invasion_risk_score\"],\n",
        "    preprocessor,\n",
        "    max_iter=20,\n",
        "    n_agents=20,\n",
        "    random_state=SEED\n",
        ")\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"\\nBest MOOA parameters:\")\n",
        "print(best_params)\n",
        "\n",
        "# Fold-wise results\n",
        "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\", \"logloss\"]\n",
        "for metric in metrics:\n",
        "    print(f\"\\n{metric.upper()}:\")\n",
        "    for i, score in enumerate(best_cv_results[metric], 1):\n",
        "        print(f\"Fold-{i}: {score:.4f}\")\n",
        "    print(f\"Mean {metric.upper()}: {np.mean(best_cv_results[metric]):.4f}\")\n",
        "\n",
        "# Generalization gap\n",
        "gen_gap = np.array(best_cv_results[\"train_accuracy\"]) - np.array(best_cv_results[\"accuracy\"])\n",
        "print(\"\\nGENERALIZATION GAP (Train - Validation Accuracy) per fold:\")\n",
        "for i, gap in enumerate(gen_gap, 1):\n",
        "    print(f\"Fold-{i}: {gap:.4f}\")\n",
        "print(f\"Mean Generalization Gap: {np.mean(gen_gap):.4f}\")\n",
        "\n",
        "# Timing stats\n",
        "total_time = end_time - start_time\n",
        "num_iterations = 15\n",
        "time_per_iteration = total_time / num_iterations\n",
        "val_scores = best_cv_results[\"accuracy\"]\n",
        "variance = np.var(val_scores)\n",
        "\n",
        "print(\"\\nEXPERIMENT 2 - TUNING STATISTICS\")\n",
        "print(f\"Total tuning time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "print(f\"Number of iterations/folds: {num_iterations}\")\n",
        "print(f\"Time per iteration: {time_per_iteration:.2f} seconds\")\n",
        "print(f\"Validation score mean: {np.mean(val_scores):.4f}\")\n",
        "print(f\"Validation score variance: {variance:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "jV3SXxhf-b8c",
        "outputId": "c35c4d53-3437-4983-807e-ebd4dd7de85d"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'reg_alpha'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1888923113.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_child_weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmax_delta_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_delta_step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mreg_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reg_alpha\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mreg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reg_lambda\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_pos_weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'reg_alpha'"
          ]
        }
      ],
      "source": [
        "# -----------------------\n",
        "# 8. Final Train on full train_set + Test eval\n",
        "# -----------------------\n",
        "X_train_full = train_set.drop(columns=[\"invasion_risk_score\", \"status\"])  # drop status too\n",
        "y_train_full = discretize_risk(train_set[\"invasion_risk_score\"])\n",
        "\n",
        "# Fit preprocessor on full training data (so target encoder sees full training labels)\n",
        "X_train_full_num = preprocessor.fit_transform(X_train_full, y_train_full)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=SEED)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_full_num, y_train_full)\n",
        "\n",
        "# Final model with best MOOA params\n",
        "final_model = xgb.XGBClassifier(\n",
        "    n_estimators=int(best_params[\"n_estimators\"]),\n",
        "    learning_rate=best_params[\"learning_rate\"],\n",
        "    max_depth=int(best_params[\"max_depth\"]),\n",
        "    subsample=best_params[\"subsample\"],\n",
        "    colsample_bytree=best_params[\"colsample_bytree\"],\n",
        "    gamma=best_params[\"gamma\"],\n",
        "    min_child_weight=int(best_params[\"min_child_weight\"]),\n",
        "    max_delta_step=int(best_params[\"max_delta_step\"]),\n",
        "    reg_alpha=best_params[\"reg_alpha\"],\n",
        "    reg_lambda=best_params[\"reg_lambda\"],\n",
        "    scale_pos_weight=best_params[\"scale_pos_weight\"],\n",
        "    objective=\"multi:softprob\",\n",
        "    eval_metric=\"mlogloss\",\n",
        "    use_label_encoder=False,\n",
        "    random_state=SEED,\n",
        "    verbosity=0\n",
        ")\n",
        "final_model.fit(X_train_res, y_train_res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMvbEhpKDCvV",
        "outputId": "a11f0804-01b1-450e-e1ea-eb9b2b6c325e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Training Performance (with SMOTE):\n",
            "Accuracy: 0.9937555753791257\n",
            "Precision: 0.9937593145793698\n",
            "Recall: 0.9937555753791257\n",
            "F1: 0.9937555645717745\n",
            "ROC-AUC: 0.9995797991099553\n",
            "Logloss: 0.4678019989316506\n"
          ]
        }
      ],
      "source": [
        "# -----------------------\n",
        "# Train set evaluation (on resampled training set)\n",
        "# -----------------------\n",
        "y_train_pred = final_model.predict(X_train_res)\n",
        "y_train_probs = final_model.predict_proba(X_train_res)\n",
        "\n",
        "print(\"\\nFinal Training Performance (with SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_train_res, y_train_pred))\n",
        "print(\"Precision:\", precision_score(y_train_res, y_train_pred, average=\"weighted\", zero_division=0))\n",
        "print(\"Recall:\", recall_score(y_train_res, y_train_pred, average=\"weighted\"))\n",
        "print(\"F1:\", f1_score(y_train_res, y_train_pred, average=\"weighted\"))\n",
        "print(\"ROC-AUC:\", roc_auc_score(pd.get_dummies(y_train_res), y_train_probs, multi_class=\"ovr\"))\n",
        "print(\"Logloss:\", log_loss(pd.get_dummies(y_train_res), y_train_probs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOvgufG1DH4k",
        "outputId": "16b869e3-08ff-4dc3-d07a-37da37870646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Performance:\n",
            "Accuracy: 0.7589605734767025\n",
            "Precision: 0.8965474961575878\n",
            "Recall: 0.7589605734767025\n",
            "F1: 0.8045850365107602\n",
            "ROC-AUC: 0.9329776490521436\n",
            "Logloss: 0.7297640752276385\n",
            "\n",
            "Classification report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.73      0.82       416\n",
            "           1       0.23      0.87      0.36        76\n",
            "           2       0.95      0.76      0.85       624\n",
            "\n",
            "    accuracy                           0.76      1116\n",
            "   macro avg       0.71      0.79      0.68      1116\n",
            "weighted avg       0.90      0.76      0.80      1116\n",
            "\n",
            "Confusion matrix (test):\n",
            "[[305  94  17]\n",
            " [  3  66   7]\n",
            " [ 18 130 476]]\n"
          ]
        }
      ],
      "source": [
        "# -----------------------\n",
        "# Test set evaluation\n",
        "# -----------------------\n",
        "X_test = test_set.drop(columns=[\"invasion_risk_score\", \"status\"])\n",
        "y_test = discretize_risk(test_set[\"invasion_risk_score\"])\n",
        "\n",
        "# Use preprocessor already fit on full training data\n",
        "X_test_num = preprocessor.transform(X_test)\n",
        "\n",
        "y_test_pred = final_model.predict(X_test_num)\n",
        "y_test_probs = final_model.predict_proba(X_test_num)\n",
        "\n",
        "print(\"\\nTest Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_test_pred, average=\"weighted\", zero_division=0))\n",
        "print(\"Recall:\", recall_score(y_test, y_test_pred, average=\"weighted\"))\n",
        "print(\"F1:\", f1_score(y_test, y_test_pred, average=\"weighted\"))\n",
        "try:\n",
        "    print(\"ROC-AUC:\", roc_auc_score(pd.get_dummies(y_test), y_test_probs, multi_class=\"ovr\"))\n",
        "except Exception as e:\n",
        "    print(\"ROC-AUC calculation failed:\", e)\n",
        "print(\"Logloss:\", log_loss(pd.get_dummies(y_test), y_test_probs))\n",
        "\n",
        "print(\"\\nClassification report (test):\")\n",
        "print(classification_report(y_test, y_test_pred, zero_division=0))\n",
        "print(\"Confusion matrix (test):\")\n",
        "print(confusion_matrix(y_test, y_test_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP0/f2TuW+ID6HBG7DQjVr5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}